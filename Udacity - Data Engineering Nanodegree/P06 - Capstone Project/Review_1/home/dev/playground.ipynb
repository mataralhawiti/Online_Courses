{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![](DEND-Porject-Page-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, datediff,  month, year\n",
    "from pyspark.sql.types import TimestampType, DateType, IntegerType, DoubleType\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "aws_key = os.environ['AWS_ACCESS_KEY_ID']=config.get('CREDENTIALS','AWS_ACCESS_KEY_ID')\n",
    "aws_secret = os.environ['AWS_SECRET_ACCESS_KEY']=config.get('CREDENTIALS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6dbad0dd7c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dend-capstone'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/us_ports/us_ports.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    319\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "data = s3.get_object(Bucket='dend-capstone', Key='/us_ports/us_ports.json')\n",
    "contents = data['Body'].read()\n",
    "print(contents)\n",
    "    \n",
    "# with open(\"s3a://dend-capstone/us_ports/us_ports.json\") as json_file:\n",
    "#     us_states = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o133.parquet.\n: com.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chain\n\tat com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e58698fcd9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://dend-capstone/raw_data/immigration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df_spark=spark.read.parquet(\"sas_data\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o133.parquet.\n: com.amazonaws.AmazonClientException: Unable to load AWS credentials from any provider in the chain\n\tat com.amazonaws.auth.AWSCredentialsProviderChain.getCredentials(AWSCredentialsProviderChain.java:117)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3521)\n\tat com.amazonaws.services.s3.AmazonS3Client.headBucket(AmazonS3Client.java:1031)\n\tat com.amazonaws.services.s3.AmazonS3Client.doesBucketExist(AmazonS3Client.java:994)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:297)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df_spark.write.parquet(\"s3a://dend-capstone/raw_data/immigration\", mode=\"overwrite\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.select(['cicid', 'i94yr', 'i94mon', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'gender', 'airline', 'visatype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|    cicid| i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|5748517.0|2016.0|   4.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|     F|     QF|      B1|\n",
      "|5748518.0|2016.0|   4.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|     F|     VA|      B1|\n",
      "|5748519.0|2016.0|   4.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|     M|     DL|      B1|\n",
      "|5748520.0|2016.0|   4.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|     F|     DL|      B1|\n",
      "|5748521.0|2016.0|   4.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|     M|     DL|      B1|\n",
      "+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"cicid\", df_spark[\"cicid\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"i94yr\", df_spark[\"i94yr\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"i94mon\", df_spark[\"i94mon\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"i94res\", df_spark[\"i94res\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"arrdate\", df_spark[\"arrdate\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"i94mode\", df_spark[\"i94mode\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"depdate\", df_spark[\"depdate\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"i94bir\", df_spark[\"i94bir\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"i94visa\", df_spark[\"i94visa\"].cast(IntegerType())) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- i94mode: integer (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i94bir: integer (nullable = true)\n",
      " |-- i94visa: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|5748517| 2016|     4|   438|    LOS|  20574|      1|     CA|  20582|    40|      1|     F|     QF|      B1|\n",
      "|5748518| 2016|     4|   438|    LOS|  20574|      1|     NV|  20591|    32|      1|     F|     VA|      B1|\n",
      "|5748519| 2016|     4|   438|    LOS|  20574|      1|     WA|  20582|    29|      1|     M|     DL|      B1|\n",
      "|5748520| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    29|      1|     F|     DL|      B1|\n",
      "|5748521| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    28|      1|     M|     DL|      B1|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44413132/count-the-number-of-missing-values-in-a-dataframe-spark\n",
    "from pyspark.sql.functions import col,sum\n",
    "Null_stats = df_spark.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_spark.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|cicid|i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-----+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|    0|    0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|414269|  83627|       0|\n",
      "+-----+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Null_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# drop all nulls\n",
    "df_spark = df_spark.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2384915"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|cicid|i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-----+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|    0|    0|     0|     0|      0|      0|      0|      0|      0|     0|      0|     0|      0|       0|\n",
      "+-----+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_Null_stats = df_spark.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_spark.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|5748517| 2016|     4|   438|    LOS|  20574|      1|     CA|  20582|    40|      1|     F|     QF|      B1|\n",
      "|5748518| 2016|     4|   438|    LOS|  20574|      1|     NV|  20591|    32|      1|     F|     VA|      B1|\n",
      "|5748519| 2016|     4|   438|    LOS|  20574|      1|     WA|  20582|    29|      1|     M|     DL|      B1|\n",
      "|5748520| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    29|      1|     F|     DL|      B1|\n",
      "|5748521| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    28|      1|     M|     DL|      B1|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94addr|\n",
      "+-------+\n",
      "|     .N|\n",
      "|     CI|\n",
      "|     TC|\n",
      "|     SC|\n",
      "|     AZ|\n",
      "|     NS|\n",
      "|     SL|\n",
      "|     LA|\n",
      "|     NL|\n",
      "|     MN|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('i94addr').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94addr_df  = pd.read_json('raw_data/I94addr.json', typ='series').to_frame(\"state\").reset_index().rename(columns={\"index\": \"code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#i94addr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ls = list(i94addr_df['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_state = udf(lambda x: '99' if x not in list(i94addr_df['code']) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"i94addr\", valid_state(df_spark.i94addr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94addr|\n",
      "+-------+\n",
      "|     SC|\n",
      "|     AZ|\n",
      "|     LA|\n",
      "|     MN|\n",
      "|     NJ|\n",
      "|     DC|\n",
      "|     OR|\n",
      "|     99|\n",
      "|     VA|\n",
      "|     RI|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('i94addr').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|5748517| 2016|     4|   438|    LOS|  20574|      1|     CA|  20582|    40|      1|     F|     QF|      B1|\n",
      "|5748518| 2016|     4|   438|    LOS|  20574|      1|     NV|  20591|    32|      1|     F|     VA|      B1|\n",
      "|5748519| 2016|     4|   438|    LOS|  20574|      1|     WA|  20582|    29|      1|     M|     DL|      B1|\n",
      "|5748520| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    29|      1|     F|     DL|      B1|\n",
      "|5748521| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    28|      1|     M|     DL|      B1|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94addr|\n",
      "+-------+\n",
      "|     SC|\n",
      "|     AZ|\n",
      "|     LA|\n",
      "|     MN|\n",
      "|     NJ|\n",
      "|     DC|\n",
      "|     OR|\n",
      "|     99|\n",
      "|     VA|\n",
      "|     RI|\n",
      "|     KY|\n",
      "|     WY|\n",
      "|     NH|\n",
      "|     MI|\n",
      "|     NV|\n",
      "|     WI|\n",
      "|     ID|\n",
      "|     CA|\n",
      "|     CT|\n",
      "|     NE|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('i94addr').distinct().show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94mode|\n",
      "+-------+\n",
      "|      1|\n",
      "|      3|\n",
      "|      9|\n",
      "|      2|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('i94mode').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "|     U|\n",
      "|     X|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('gender').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|5748517| 2016|     4|   438|    LOS|  20574|      1|     CA|  20582|    40|      1|     F|     QF|      B1|\n",
      "|5748518| 2016|     4|   438|    LOS|  20574|      1|     NV|  20591|    32|      1|     F|     VA|      B1|\n",
      "|5748519| 2016|     4|   438|    LOS|  20574|      1|     WA|  20582|    29|      1|     M|     DL|      B1|\n",
      "|5748520| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    29|      1|     F|     DL|      B1|\n",
      "|5748521| 2016|     4|   438|    LOS|  20574|      1|     WA|  20588|    28|      1|     M|     DL|      B1|\n",
      "+-------+-----+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# let's convert SAS date format to date .. I follow :\n",
    "#https://stackoverflow.com/questions/36412864/convert-numeric-sas-date-to-datetime-in-pandas\n",
    "#https://communities.sas.com/t5/Developers/How-can-I-read-a-SAS-datetime-value-in-Python/td-p/433593#\n",
    "# from pyspark.sql.types import DateType\n",
    "# from pyspark.sql.functions import udf, col, to_timestamp\n",
    "\n",
    "sasdate_to_date = udf(lambda x: ( pd.to_timedelta(x, unit='D') + pd.Timestamp('1960-1-1') ).date(), DateType() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"arrdate\", sasdate_to_date(df_spark.arrdate)) \\\n",
    "                    .withColumn(\"depdate\", sasdate_to_date(df_spark.depdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+-------+------+-------+--------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+-------+------+-------+--------+\n",
      "|5748517| 2016|     4|   438|    LOS|2016-04-30|      1|     CA|2016-05-08|    40|      1|     F|     QF|      B1|\n",
      "|5748518| 2016|     4|   438|    LOS|2016-04-30|      1|     NV|2016-05-17|    32|      1|     F|     VA|      B1|\n",
      "|5748519| 2016|     4|   438|    LOS|2016-04-30|      1|     WA|2016-05-08|    29|      1|     M|     DL|      B1|\n",
      "|5748520| 2016|     4|   438|    LOS|2016-04-30|      1|     WA|2016-05-14|    29|      1|     F|     DL|      B1|\n",
      "|5748521| 2016|     4|   438|    LOS|2016-04-30|      1|     WA|2016-05-14|    28|      1|     M|     DL|      B1|\n",
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df_spark = df_spark.withColumn(\"stay_length\", datediff(df_spark.depdate, df_spark.arrdate) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+-------+------+-------+--------+-----------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir|i94visa|gender|airline|visatype|stay_length|\n",
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+-------+------+-------+--------+-----------+\n",
      "|5748517| 2016|     4|   438|    LOS|2016-04-30|      1|     CA|2016-05-08|    40|      1|     F|     QF|      B1|          8|\n",
      "|5748518| 2016|     4|   438|    LOS|2016-04-30|      1|     NV|2016-05-17|    32|      1|     F|     VA|      B1|         17|\n",
      "|5748519| 2016|     4|   438|    LOS|2016-04-30|      1|     WA|2016-05-08|    29|      1|     M|     DL|      B1|          8|\n",
      "|5748520| 2016|     4|   438|    LOS|2016-04-30|      1|     WA|2016-05-14|    29|      1|     F|     DL|      B1|         14|\n",
      "|5748521| 2016|     4|   438|    LOS|2016-04-30|      1|     WA|2016-05-14|    28|      1|     M|     DL|      B1|         14|\n",
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+-------+------+-------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df_spark = df_spark.withColumn(\"i94mode\",when(df_spark.i94mode == 1, 'Air')\\\n",
    "                               .when(df_spark.i94mode == 2, 'Sea')\\\n",
    "                               .when(df_spark.i94mode == 3, 'Land')\\\n",
    "                               .otherwise('other'))\n",
    "\n",
    "  \n",
    "#df_spark = df_spark.withColumn(\"i94mode\", df_spark[\"i94mode\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"i94visa\",when(df_spark.i94visa == 1, 'Business')\\\n",
    "                               .when(df_spark.i94visa == 2, 'Pleasure')\\\n",
    "                               .when(df_spark.i94visa == 3, 'Student')\\\n",
    "                               .otherwise('other'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94mode|\n",
      "+-------+\n",
      "|    Sea|\n",
      "|  other|\n",
      "|   Land|\n",
      "|    Air|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.select('i94mode').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+--------+------+-------+--------+-----------+\n",
      "|  cicid|i94yr|i94mon|i94res|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir| i94visa|gender|airline|visatype|stay_length|\n",
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+--------+------+-------+--------+-----------+\n",
      "|5748517| 2016|     4|   438|    LOS|2016-04-30|    Air|     CA|2016-05-08|    40|Business|     F|     QF|      B1|          8|\n",
      "|5748518| 2016|     4|   438|    LOS|2016-04-30|    Air|     NV|2016-05-17|    32|Business|     F|     VA|      B1|         17|\n",
      "|5748519| 2016|     4|   438|    LOS|2016-04-30|    Air|     WA|2016-05-08|    29|Business|     M|     DL|      B1|          8|\n",
      "|5748520| 2016|     4|   438|    LOS|2016-04-30|    Air|     WA|2016-05-14|    29|Business|     F|     DL|      B1|         14|\n",
      "|5748521| 2016|     4|   438|    LOS|2016-04-30|    Air|     WA|2016-05-14|    28|Business|     M|     DL|      B1|         14|\n",
      "+-------+-----+------+------+-------+----------+-------+-------+----------+------+--------+------+-------+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- I'll use the provided dataset sample in csv format to explore I94 Immigration data, and make some assumtions before working with the full dataset in Parquet format.\n",
    "- Using the dataset sample I'll determine the needed columns, and the required cleansing steps, then apply that on the full parquet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "img_df = pd.read_csv(\"immigration_data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port',\n",
       "       'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa',\n",
       "       'count', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2027561</td>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2171295</td>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160423</td>\n",
       "      <td>MTR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>589494</td>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2631158</td>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>DOH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032257</td>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>721257</td>\n",
       "      <td>1481650.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>577.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20552.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>GA</td>\n",
       "      <td>20606.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>10072016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DL</td>\n",
       "      <td>7.368526e+08</td>\n",
       "      <td>910</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1072780</td>\n",
       "      <td>2197173.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20556.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20635.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>10112016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CX</td>\n",
       "      <td>7.863122e+08</td>\n",
       "      <td>870</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112205</td>\n",
       "      <td>232708.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20546.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>06302016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA</td>\n",
       "      <td>5.547449e+10</td>\n",
       "      <td>00117</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2577162</td>\n",
       "      <td>5227851.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20575.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>07262016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LX</td>\n",
       "      <td>5.941342e+10</td>\n",
       "      <td>00008</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10930</td>\n",
       "      <td>13213.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>06292016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>5.544979e+10</td>\n",
       "      <td>00109</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  \\\n",
       "0     2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0   \n",
       "1     2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0   \n",
       "2      589494  1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0   \n",
       "3     2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0   \n",
       "4     3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0   \n",
       "5      721257  1481650.0  2016.0     4.0   577.0   577.0     ATL  20552.0   \n",
       "6     1072780  2197173.0  2016.0     4.0   245.0   245.0     SFR  20556.0   \n",
       "7      112205   232708.0  2016.0     4.0   113.0   135.0     NYC  20546.0   \n",
       "8     2577162  5227851.0  2016.0     4.0   131.0   131.0     CHI  20572.0   \n",
       "9       10930    13213.0  2016.0     4.0   116.0   116.0     LOS  20545.0   \n",
       "\n",
       "   i94mode i94addr  depdate  i94bir  i94visa  count  dtadfile visapost occup  \\\n",
       "0      1.0      HI  20573.0    61.0      2.0    1.0  20160422      NaN   NaN   \n",
       "1      1.0      TX  20568.0    26.0      2.0    1.0  20160423      MTR   NaN   \n",
       "2      1.0      FL  20571.0    76.0      2.0    1.0  20160407      NaN   NaN   \n",
       "3      1.0      CA  20581.0    25.0      2.0    1.0  20160428      DOH   NaN   \n",
       "4      3.0      NY  20553.0    19.0      2.0    1.0  20160406      NaN   NaN   \n",
       "5      1.0      GA  20606.0    51.0      2.0    1.0  20160408      NaN   NaN   \n",
       "6      1.0      CA  20635.0    48.0      2.0    1.0  20160412      NaN   NaN   \n",
       "7      1.0      NY  20554.0    33.0      2.0    1.0  20160402      NaN   NaN   \n",
       "8      1.0      IL  20575.0    39.0      2.0    1.0  20160428      NaN   NaN   \n",
       "9      1.0      CA  20553.0    35.0      2.0    1.0  20160401      NaN   NaN   \n",
       "\n",
       "  entdepa entdepd  entdepu matflag  biryear   dtaddto gender  insnum airline  \\\n",
       "0       G       O      NaN       M   1955.0  07202016      F     NaN      JL   \n",
       "1       G       R      NaN       M   1990.0  10222016      M     NaN     *GA   \n",
       "2       G       O      NaN       M   1940.0  07052016      M     NaN      LH   \n",
       "3       G       O      NaN       M   1991.0  10272016      M     NaN      QR   \n",
       "4       Z       K      NaN       M   1997.0  07042016      F     NaN     NaN   \n",
       "5       T       N      NaN       M   1965.0  10072016      M     NaN      DL   \n",
       "6       T       O      NaN       M   1968.0  10112016      F     NaN      CX   \n",
       "7       G       O      NaN       M   1983.0  06302016      F     NaN      BA   \n",
       "8       O       O      NaN       M   1977.0  07262016    NaN     NaN      LX   \n",
       "9       O       O      NaN       M   1981.0  06292016    NaN     NaN      AA   \n",
       "\n",
       "         admnum  fltno visatype  \n",
       "0  5.658267e+10  00782       WT  \n",
       "1  9.436200e+10  XBLNG       B2  \n",
       "2  5.578047e+10  00464       WT  \n",
       "3  9.478970e+10  00739       B2  \n",
       "4  4.232257e+10   LAND       WT  \n",
       "5  7.368526e+08    910       B2  \n",
       "6  7.863122e+08    870       B2  \n",
       "7  5.547449e+10  00117       WT  \n",
       "8  5.941342e+10  00008       WT  \n",
       "9  5.544979e+10  00109       WT  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_cloumns = [ 'cicid', 'i94yr', 'i94mon', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'gender', 'airline', 'visatype']\n",
    "invalid_columns = ['Unnamed: 0','i94cit', 'arrdate','count',  'dtadfile', 'visapost', 'occup','entdepa', 'entdepd','entdepu','matflag', 'biryear','dtaddto', 'insnum','admnum', 'fltno' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_df = img_df[['cicid', 'i94yr', 'i94mon', 'i94res', 'i94port', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'gender', 'airline', 'visatype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>gender</th>\n",
       "      <th>airline</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>JL</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>*GA</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>LH</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>M</td>\n",
       "      <td>QR</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cicid   i94yr  i94mon  i94res i94port  i94mode i94addr  depdate  \\\n",
       "0  4084316.0  2016.0     4.0   209.0     HHW      1.0      HI  20573.0   \n",
       "1  4422636.0  2016.0     4.0   582.0     MCA      1.0      TX  20568.0   \n",
       "2  1195600.0  2016.0     4.0   112.0     OGG      1.0      FL  20571.0   \n",
       "3  5291768.0  2016.0     4.0   297.0     LOS      1.0      CA  20581.0   \n",
       "4   985523.0  2016.0     4.0   111.0     CHM      3.0      NY  20553.0   \n",
       "\n",
       "   i94bir  i94visa gender airline visatype  \n",
       "0    61.0      2.0      F      JL       WT  \n",
       "1    26.0      2.0      M     *GA       B2  \n",
       "2    76.0      2.0      M      LH       WT  \n",
       "3    25.0      2.0      M      QR       B2  \n",
       "4    19.0      2.0      F     NaN       WT  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airline IATA Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/BesrourMS/Airlines\n",
    "#i94addr_df  = pd.read_json('raw_data/I94addr.json', typ='series').to_frame(\"state\").reset_index().rename(columns={\"index\": \"code\"})\n",
    "# airline_df  = pd.read_json('raw_date/airlines_iata_codes.json', typ='series')\n",
    "# airline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0A</td>\n",
       "      <td>Amber Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0B</td>\n",
       "      <td>Blue Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0D</td>\n",
       "      <td>Darwin Airline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2B</td>\n",
       "      <td>Aerocondor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2C</td>\n",
       "      <td>SNCF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata            name\n",
       "0   0A       Amber Air\n",
       "1   0B        Blue Air\n",
       "2   0D  Darwin Airline\n",
       "3   2B      Aerocondor\n",
       "4   2C            SNCF"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "airline_df  = pd.read_json('raw_data/airlines_iata_codes.json')\n",
    "airline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_tmp = spark.read.csv('raw_data/GlobalLandTemperaturesByState.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645675"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_tmp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01|            25.544|                        1.171| Acre| Brazil|\n",
      "|1855-06-01|            24.228|                        1.103| Acre| Brazil|\n",
      "|1855-07-01|            24.371|                        1.044| Acre| Brazil|\n",
      "|1855-08-01|            25.427|                        1.073| Acre| Brazil|\n",
      "|1855-09-01|            25.675|                        1.014| Acre| Brazil|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_tmp = spark_tmp.withColumn(\"dt\", spark_tmp[\"dt\"].cast(DateType())) \\\n",
    "                        .withColumnRenamed(\"dt\", \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_tmp = spark_tmp.filter((spark_tmp.Country == 'United States') & (spark_tmp.date >= '2011-01-01')).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+--------------+-------------+\n",
      "|      date|AverageTemperature|AverageTemperatureUncertainty|         State|      Country|\n",
      "+----------+------------------+-----------------------------+--------------+-------------+\n",
      "|2012-10-01|            17.541|                        0.236|       Arizona|United States|\n",
      "|2012-03-01|            21.678|                        0.211|       Florida|United States|\n",
      "|2011-01-01|             -5.21|                         0.29|         Idaho|United States|\n",
      "|2013-07-01|            20.621|                         0.29|      Michigan|United States|\n",
      "|2012-03-01|            17.774|                         0.16|South Carolina|United States|\n",
      "+----------+------------------+-----------------------------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "spark_tmp = spark_tmp.select(['date', 'AverageTemperature', 'State']) \\\n",
    ".withColumn(\"month\", month(spark_tmp[\"date\"])) \\\n",
    ".withColumn(\"year\", year(spark_tmp[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+-----+----+\n",
      "|      date|AverageTemperature|         State|month|year|\n",
      "+----------+------------------+--------------+-----+----+\n",
      "|2012-10-01|            17.541|       Arizona|   10|2012|\n",
      "|2012-03-01|            21.678|       Florida|    3|2012|\n",
      "|2011-01-01|             -5.21|         Idaho|    1|2011|\n",
      "|2013-07-01|            20.621|      Michigan|    7|2013|\n",
      "|2012-03-01|            17.774|South Carolina|    3|2012|\n",
      "+----------+------------------+--------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, to_timestamp, datediff, when, month, year\n",
    "spark_tmp = spark_tmp.withColumn(\"State\",when(spark_tmp.State == 'Georgia (State)', 'Georgia').otherwise(spark_tmp.State))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('raw_data/I94addr.json') as json_file:\n",
    "    us_states = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_abbr = udf( lambda x: {k.upper(): v.upper() for k, v in us_states.items()}[x.upper()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+-----+----+\n",
      "|      date|AverageTemperature|         State|month|year|\n",
      "+----------+------------------+--------------+-----+----+\n",
      "|2012-10-01|            17.541|       Arizona|   10|2012|\n",
      "|2012-03-01|            21.678|       Florida|    3|2012|\n",
      "|2011-01-01|             -5.21|         Idaho|    1|2011|\n",
      "|2013-07-01|            20.621|      Michigan|    7|2013|\n",
      "|2012-03-01|            17.774|South Carolina|    3|2012|\n",
      "+----------+------------------+--------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_tmp = spark_tmp.withColumn(\"State\",  state_abbr(spark_tmp.State))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+-----+----+\n",
      "|      date|AverageTemperature|State|month|year|\n",
      "+----------+------------------+-----+-----+----+\n",
      "|2012-10-01|            17.541|   AZ|   10|2012|\n",
      "|2012-03-01|            21.678|   FL|    3|2012|\n",
      "|2011-01-01|             -5.21|   ID|    1|2011|\n",
      "|2013-07-01|            20.621|   MI|    7|2013|\n",
      "|2012-03-01|            17.774|   SC|    3|2012|\n",
      "|2012-10-01| 7.752999999999999|   SD|   10|2012|\n",
      "|2013-08-01|             24.06|   TN|    8|2013|\n",
      "|2011-01-01|            -1.169|   DE|    1|2011|\n",
      "|2013-04-01|             4.252|   ID|    4|2013|\n",
      "|2013-06-01|            22.248|   IN|    6|2013|\n",
      "|2012-11-01|0.8579999999999999|   ME|   11|2012|\n",
      "|2011-08-01|29.011000000000006|   MS|    8|2011|\n",
      "|2013-07-01|            23.124|   NM|    7|2013|\n",
      "|2012-03-01|              3.86|   OR|    3|2012|\n",
      "|2012-06-01|            24.301|   SC|    6|2012|\n",
      "|2012-08-01|            20.008|   WI|    8|2012|\n",
      "|2012-07-01|            26.629|   DC|    7|2012|\n",
      "|2011-06-01|21.691999999999997|   IA|    6|2011|\n",
      "|2012-05-01|            19.045|   IA|    5|2012|\n",
      "|2012-03-01|1.2199999999999998|   ME|    3|2012|\n",
      "+----------+------------------+-----+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_tmp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('raw_data/airport-codes_csv.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_air.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "| 01AK|     heliport|Providence Seward...|         120|       NA|         US|     US-AK|      Seward|    01AK|     null|      01AK|-149.446249008, 6...|\n",
      "| 04WA|small_airport|  Ox Meadows Airport|        2345|       NA|         US|     US-WA|     Spokane|    04WA|     null|      04WA|-117.43699646, 47...|\n",
      "| 08OR|small_airport| Saxon Sycan Airport|        4990|       NA|         US|     US-OR| Silver Lake|    08OR|     null|      08OR|-121.116996765136...|\n",
      "| 0CD9|     heliport|Swedish S.W. Medi...|        5563|       NA|         US|     US-CO|   Littleton|    0CD9|     null|      0CD9|-105.091003418, 3...|\n",
      "| 0GA3|small_airport|   Ayresouth Airport|        1287|       NA|         US|     US-GA|      Temple|    0GA3|     null|      0GA3|-85.0607986450195...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# columns\n",
    "df_spark = df_spark.filter(df_spark.iso_country == 'US').select(['ident', 'type', 'name', 'elevation_ft', 'iso_region', 'municipality', 'iata_code', 'local_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "|ident|         type|                name|elevation_ft|iso_region|municipality|iata_code|local_code|\n",
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "| 01AK|     heliport|Providence Seward...|         120|     US-AK|      Seward|     null|      01AK|\n",
      "| 04WA|small_airport|  Ox Meadows Airport|        2345|     US-WA|     Spokane|     null|      04WA|\n",
      "| 08OR|small_airport| Saxon Sycan Airport|        4990|     US-OR| Silver Lake|     null|      08OR|\n",
      "| 0CD9|     heliport|Swedish S.W. Medi...|        5563|     US-CO|   Littleton|     null|      0CD9|\n",
      "| 0GA3|small_airport|   Ayresouth Airport|        1287|     US-GA|      Temple|     null|      0GA3|\n",
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22757"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_air.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "| large_airport|\n",
      "|   balloonport|\n",
      "| seaplane_base|\n",
      "|      heliport|\n",
      "|        closed|\n",
      "|medium_airport|\n",
      "| small_airport|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.select('type').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spark_air.filter(spark_air.type == 'small_airport').show(10)\n",
    "#string[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_air = spark_air.withColumn(\"iso_region\", spark_air.iso_region[-2:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spark_air.select('iso_region').distinct().show(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "|ident|         type|                name|elevation_ft|iso_region|municipality|iata_code|local_code|\n",
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "| 01AK|     heliport|Providence Seward...|         120|        AK|      Seward|     null|      01AK|\n",
      "| 04WA|small_airport|  Ox Meadows Airport|        2345|        WA|     Spokane|     null|      04WA|\n",
      "| 08OR|small_airport| Saxon Sycan Airport|        4990|        OR| Silver Lake|     null|      08OR|\n",
      "| 0CD9|     heliport|Swedish S.W. Medi...|        5563|        CO|   Littleton|     null|      0CD9|\n",
      "| 0GA3|small_airport|   Ayresouth Airport|        1287|        GA|      Temple|     null|      0GA3|\n",
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spark_air.filter(spark_air.type == 'large_airport').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_air = spark_air.withColumn(\"elevation_ft\", spark_air[\"elevation_ft\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dm_spark = spark.read.csv('raw_data/us-cities-demographics.csv', sep = ';', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dm_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dm_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumnRenamed(\"Median Age\", \"Median_Age\")\\\n",
    "                    .withColumnRenamed(\"Male Population\", \"Male_Population\")\\\n",
    "                    .withColumnRenamed(\"Female Population\", \"Female_Population\")\\\n",
    "                    .withColumnRenamed(\"Total Population\", \"Total_Population\")\\\n",
    "                    .withColumnRenamed(\"Foreign-born\", \"Foreign_born\")\\\n",
    "                    .withColumnRenamed(\"State Code\", \"State_Code\")\\\n",
    "                    .withColumn(\"Count\", df_spark[\"Count\"].cast(IntegerType())) \\\n",
    "                    .select(['City', 'State', 'Median_Age', 'Male_Population',\\\n",
    "                             'Female_Population', 'Total_Population', 'Foreign_born', 'State_Code',\\\n",
    "                             'Race', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dm_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, DateType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"Median_Age\", df_spark[\"Median_Age\"].cast(DoubleType())) \\\n",
    "                    .withColumn(\"Male_Population\", df_spark[\"Male_Population\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Female_Population\", df_spark[\"Female_Population\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Total_Population\", df_spark[\"Total_Population\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Foreign_born\", df_spark[\"Foreign_born\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Count\", df_spark[\"Count\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dm_spark.filter((dm_spark.State_Code == 'MD') & (dm_spark.City == 'Gaithersburg')).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dm_spark.filter((dm_spark.State_Code == 'MD') & ( (dm_spark.City == 'Baltimore') | (dm_spark.City == 'Gaithersburg') )).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tm = dm_spark.filter((dm_spark.State_Code == 'MD') & ( (dm_spark.City == 'Baltimore') | (dm_spark.City == 'Gaithersburg') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tm = dm_spark.filter((dm_spark.State_Code == 'MD') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#tm.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_pivot = dm_spark.groupBy(['City', 'State', 'Median_Age', 'Male_Population',\\\n",
    "                             'Female_Population', 'Total_Population', 'Foreign_born', 'State_Code']).pivot(\"Race\").sum(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_pivot = df_pivot.withColumnRenamed(\"American Indian and Alaska Native\", \"Native\")\\\n",
    ".withColumnRenamed(\"Black or African-American\", \"Black\")\\\n",
    ".withColumnRenamed(\"Hispanic or Latino\", \"Hispanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+---------------+-----------------+----------------+------------+----------+------+-----+-----+--------+-----+\n",
      "|             City|     State|Median_Age|Male_Population|Female_Population|Total_Population|Foreign_born|State_Code|Native|Asian|Black|Hispanic|White|\n",
      "+-----------------+----------+----------+---------------+-----------------+----------------+------------+----------+------+-----+-----+--------+-----+\n",
      "|    Coral Springs|   Florida|      37.2|          63316|            66186|          129502|       38552|        FL|  1262| 6573|27225|   40817|90896|\n",
      "|Arlington Heights|  Illinois|      45.9|          37627|            39369|           76996|       14630|        IL|  null| 9975| 1222|    2742|66285|\n",
      "|    Santa Barbara|California|      37.8|          45068|            46784|           91852|       19441|        CA|  1560| 4964| 2013|   31102|73659|\n",
      "+-----------------+----------+----------+---------------+-----------------+----------------+------------+----------+------+-----+-----+--------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivot.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_pivot.createOrReplaceTempView(\"pivot_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "qry = spark.sql(\"\"\"\n",
    "        SELECT State_Code, \n",
    "        cast ( avg(Median_Age) as DECIMAL(4,2) ) as age,\n",
    "        sum(Male_Population) as Male_Population,\n",
    "        sum(Female_Population) as Female_Population,\n",
    "        sum(Total_Population) as Total_Population, \n",
    "        sum(Foreign_born) as Foreign_born,\n",
    "        sum(Native) as Native,\n",
    "        sum(Asian) as Asian,\n",
    "        sum(Black) as Black,\n",
    "        sum(White) as White\n",
    "        FROM pivot_table\n",
    "        GROUP BY State_Code\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+-------+-------+------+------+------+-------+\n",
      "|State_Code|  age|   male|  femal|  total|Foreign|Native| Asian| Black|  White|\n",
      "+----------+-----+-------+-------+-------+-------+------+------+------+-------+\n",
      "|        AZ|35.04|2227455|2272087|4499542| 682313|129708|229183|296222|3591611|\n",
      "|        SC|34.18| 260944| 272713| 533657|  27744|  3705| 13355|175064| 343764|\n",
      "|        LA|34.63| 626998| 673597|1300595|  83419|  8263| 38739|602377| 654578|\n",
      "|        MN|35.62| 702157| 720246|1422403| 215873| 25242|151544|216731|1050239|\n",
      "|        NJ|35.12| 705736| 723172|1428908| 477028| 11350|116844|452202| 615083|\n",
      "+----------+-----+-------+-------+-------+-------+------+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qry.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmg = dm_spark.filter((dm_spark.State_Code == 'MD') & ( (dm_spark.City == 'Gaithersburg') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+---------------+-----------------+----------------+------------+----------+--------------------+-----+\n",
      "|        City|   State|Median_Age|Male_Population|Female_Population|Total_Population|Foreign_born|State_Code|                Race|Count|\n",
      "+------------+--------+----------+---------------+-----------------+----------------+------------+----------+--------------------+-----+\n",
      "|Gaithersburg|Maryland|      35.7|          32211|            35234|           67445|       28133|        MD|               White|36639|\n",
      "|Gaithersburg|Maryland|      35.7|          32211|            35234|           67445|       28133|        MD|Black or African-...|12482|\n",
      "|Gaithersburg|Maryland|      35.7|          32211|            35234|           67445|       28133|        MD|  Hispanic or Latino|18721|\n",
      "|Gaithersburg|Maryland|      35.7|          32211|            35234|           67445|       28133|        MD|American Indian a...|  672|\n",
      "|Gaithersburg|Maryland|      35.7|          32211|            35234|           67445|       28133|        MD|               Asian|13169|\n",
      "+------------+--------+----------+---------------+-----------------+----------------+------------+----------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmg.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tm_a = dm_spark.filter((dm_spark.State_Code == 'MD') & ( (dm_spark.City == 'Baltimore') | (dm_spark.City == 'Gaithersburg') ))\\\n",
    ".select(['City','Median_Age', 'Male_Population',\\\n",
    "                             'Female_Population', 'Total_Population', 'Foreign_born', 'State_Code',\\\n",
    "                             'Race', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-----------------+----------------+------------+----------+--------------------+------+\n",
      "|        City|Median_Age|Male_Population|Female_Population|Total_Population|Foreign_born|State_Code|                Race| Count|\n",
      "+------------+----------+---------------+-----------------+----------------+------------+----------+--------------------+------+\n",
      "|   Baltimore|      34.7|         294027|           327822|          621849|       49857|        MD|  Hispanic or Latino| 29953|\n",
      "|   Baltimore|      34.7|         294027|           327822|          621849|       49857|        MD|Black or African-...|396270|\n",
      "|Gaithersburg|      35.7|          32211|            35234|           67445|       28133|        MD|               White| 36639|\n",
      "|Gaithersburg|      35.7|          32211|            35234|           67445|       28133|        MD|Black or African-...| 12482|\n",
      "|   Baltimore|      34.7|         294027|           327822|          621849|       49857|        MD|               White|207274|\n",
      "|Gaithersburg|      35.7|          32211|            35234|           67445|       28133|        MD|  Hispanic or Latino| 18721|\n",
      "|   Baltimore|      34.7|         294027|           327822|          621849|       49857|        MD|               Asian| 20161|\n",
      "|   Baltimore|      34.7|         294027|           327822|          621849|       49857|        MD|American Indian a...|  7730|\n",
      "|Gaithersburg|      35.7|          32211|            35234|           67445|       28133|        MD|American Indian a...|   672|\n",
      "|Gaithersburg|      35.7|          32211|            35234|           67445|       28133|        MD|               Asian| 13169|\n",
      "+------------+----------+---------------+-----------------+----------------+------------+----------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm_a.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Median_Age: string (nullable = true)\n",
      " |-- Male_Population: string (nullable = true)\n",
      " |-- Female_Population: string (nullable = true)\n",
      " |-- Total_Population: string (nullable = true)\n",
      " |-- Foreign_born: string (nullable = true)\n",
      " |-- State_Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tm_a.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tm_a.createOrReplaceTempView(\"state_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "qry = spark.sql(\"\"\"\n",
    "        SELECT State_Code, Race, sum(Count) as countt, sum(Foreign_born) as fore, avg(Total_Population) as total, avg(Female_Population) as femal, avg(Male_Population) as male, avg(Median_Age) as age\n",
    "        FROM state_table\n",
    "        GROUP BY State_Code, City, Race\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------+-------+--------+--------+--------+----+\n",
      "|State_Code|              Race|  countt|   fore|   total|   femal|    male| age|\n",
      "+----------+------------------+--------+-------+--------+--------+--------+----+\n",
      "|        MD|Hispanic or Latino| 18721.0|28133.0| 67445.0| 35234.0| 32211.0|35.7|\n",
      "|        MD|             White| 36639.0|28133.0| 67445.0| 35234.0| 32211.0|35.7|\n",
      "|        MD|             Asian| 20161.0|49857.0|621849.0|327822.0|294027.0|34.7|\n",
      "|        MD|Hispanic or Latino| 29953.0|49857.0|621849.0|327822.0|294027.0|34.7|\n",
      "|        MD|             White|207274.0|49857.0|621849.0|327822.0|294027.0|34.7|\n",
      "+----------+------------------+--------+-------+--------+--------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qry.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = spark.read.csv('raw_data/us-cities-demographics.csv', sep = ';', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumnRenamed(\"Median Age\", \"Median_Age\")\\\n",
    "                    .withColumnRenamed(\"Male Population\", \"Male_Population\")\\\n",
    "                    .withColumnRenamed(\"Female Population\", \"Female_Population\")\\\n",
    "                    .withColumnRenamed(\"Total Population\", \"Total_Population\")\\\n",
    "                    .withColumnRenamed(\"Foreign-born\", \"Foreign_born\")\\\n",
    "                    .withColumnRenamed(\"State Code\", \"State_Code\")\\\n",
    "                    .withColumn(\"Count\", df_spark[\"Count\"].cast(IntegerType())) \\\n",
    "                    .select(['City', 'State', 'Median_Age', 'Male_Population',\\\n",
    "                             'Female_Population', 'Total_Population', 'Foreign_born', 'State_Code',\\\n",
    "                             'Race', 'Count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn(\"Median_Age\", df_spark[\"Median_Age\"].cast(DoubleType())) \\\n",
    "                    .withColumn(\"Male_Population\", df_spark[\"Male_Population\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Female_Population\", df_spark[\"Female_Population\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Total_Population\", df_spark[\"Total_Population\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Foreign_born\", df_spark[\"Foreign_born\"].cast(IntegerType())) \\\n",
    "                    .withColumn(\"Count\", df_spark[\"Count\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.groupBy(['City', 'State', 'Median_Age', 'Male_Population',\\\n",
    "                             'Female_Population', 'Total_Population', 'Foreign_born', \\\n",
    "                                      'State_Code'])\\\n",
    "                            .pivot(\"Race\")\\\n",
    "                            .sum(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumnRenamed(\"American Indian and Alaska Native\", \"Native\")\\\n",
    "                        .withColumnRenamed(\"Black or African-American\", \"Black\")\\\n",
    "                        .withColumnRenamed(\"Hispanic or Latino\", \"Hispanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark.createOrReplaceTempView(\"pivot_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dem_by_state = spark.sql(\"\"\"\n",
    "            SELECT State_Code, \n",
    "            cast ( avg(Median_Age) as DECIMAL(4,2) ) as age,\n",
    "            sum(Male_Population) as Male_Population,\n",
    "            sum(Female_Population) as Female_Population,\n",
    "            sum(Total_Population) as Total_Population, \n",
    "            sum(Foreign_born) as Foreign_born,\n",
    "            sum(Native) as Native,\n",
    "            sum(Asian) as Asian,\n",
    "            sum(Black) as Black,\n",
    "            sum(White) as White\n",
    "            FROM pivot_table\n",
    "            GROUP BY State_Code\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------------+-----------------+----------------+------------+------+------+------+-------+\n",
      "|State_Code|  age|Male_Population|Female_Population|Total_Population|Foreign_born|Native| Asian| Black|  White|\n",
      "+----------+-----+---------------+-----------------+----------------+------------+------+------+------+-------+\n",
      "|        AZ|35.04|        2227455|          2272087|         4499542|      682313|129708|229183|296222|3591611|\n",
      "|        SC|34.18|         260944|           272713|          533657|       27744|  3705| 13355|175064| 343764|\n",
      "|        LA|34.63|         626998|           673597|         1300595|       83419|  8263| 38739|602377| 654578|\n",
      "|        MN|35.62|         702157|           720246|         1422403|      215873| 25242|151544|216731|1050239|\n",
      "|        NJ|35.12|         705736|           723172|         1428908|      477028| 11350|116844|452202| 615083|\n",
      "+----------+-----+---------------+-----------------+----------------+------------+------+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dem_by_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dem_by_state.write.parquet(os.path.join(\"s3a://dend-capstone/\",\"us_demographic\"), mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### I94 Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. I'll group data by countrt where imigrant comes from, and US state where immgirants enteres US\n",
    "2. Remove all records where country (xxx) is Null\n",
    "3. After understaning the dataset, I came up with list of columns that will server my project purpose as definded abov.\n",
    "4. All undesired columns will be removed.\n",
    "5. For the column (I94MODE), the codes will be replaced with the values.\n",
    "6. For the column (I94VISA), the codes will be replaced with the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. I'll filter dataset to include only US data\n",
    "2. I'll filter dataset to include only > 2011 data or above.\n",
    "3. I'll extract year and month from date\n",
    "4. Convert to the appropriate data type\n",
    "5. drop uneccly columns\n",
    "6. repalce State names with abbrivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Code Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5c6b4d5d9db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
