{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Scope "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Overview :\n",
    "In this project, I want to build a data lake that contain all neccessrry data that could help interested indivuals to get better understanding about the flow of immigrants into the United States. It could help to answer questions like :\n",
    "- Where most immigrants come from?\n",
    "- Whic US states have the majority of the immigrants ?\n",
    "- Does the tempratiure play major role in immigration distribuation across US ?\n",
    "- Which US airports has most number of immgiration's arrivals ?\n",
    "- what's the prefered airlines for immigrants to get to US ?\n",
    "- Is US airlines benefiting from immigrants moving to US ?\n",
    "\n",
    "Further, this data lake can be used as a starting point to build very well designed and comperhanisve DW.\n",
    "\n",
    "\n",
    "#### Architecture and Technology\n",
    "All our data sources will be placed in ***AWS S3 bucket***, and then I'll utliz ***AWS EMR*** running ***Apache Spark*** to move, transform and clean the data. Finally, the processed data will be written out to ***AWS S3 bucket*** where clean data will placed in different directories according to our data model. Our data pipeline will be orchestrated using ***Apache Airflow***\n",
    "\n",
    "\n",
    "<img src=\"images/DEND-Porject-Page-2.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Describe and Gather Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***I94 Immigration Data*** \n",
    "- **Source** : US National Tourism and Trade Office.\n",
    "- **Link** : <a href=\"https://travel.trade.gov/research/reports/i94/historical/2016.html\">Click here</a>\n",
    "- **Description** : dataset contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "- **Format** : originally in *sas7bdat* but will be converted to *parquet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***World Temperature Data***\n",
    "- **Source** : Kaggle\n",
    "- **Link** : <a href=\"https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\">Click here</a>\n",
    "- **Description** : dataset contains monthly average temperature per country per city per state for the period between 1899to 2013 \n",
    "- **Format** : csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***U.S. City Demographic Data***\n",
    "- **Source** : OpenSoft\n",
    "- **Link** : <a href=\"https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\">Click here</a>\n",
    "- **Description** : dataset contains information about the demographics of all US cities and census-designated places with a population greater than or equal to 65,000.  This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "- **Format** : csv/JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Airport Code Table*** \n",
    "- **Source** : datahub\n",
    "- **Link** : <a href=\"https://datahub.io/core/airport-codes#data\">Click here</a>\n",
    "- **Description** : dataset contains world wide airport codes and corresponding cities\n",
    "- **Format** : csv/JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For more detailed data exploratry code please refer to notebook **dev/playground.ipynb**.\n",
    "First, I built my exploring/final code in that notebook, then moved it and organized it in better python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All imports here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, sum, to_timestamp, datediff,  month, year\n",
    "from pyspark.sql.types import TimestampType, DateType, IntegerType, DoubleType\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "aws_key = os.environ['AWS_ACCESS_KEY_ID']=config.get('CREDENTIALS','AWS_ACCESS_KEY_ID')\n",
    "aws_secret = os.environ['AWS_SECRET_ACCESS_KEY']=config.get('CREDENTIALS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***I94 Immigration Data*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Exploring****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark =spark.read.format('com.github.saurfang.sas.spark')\\\n",
    ".load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row counts \n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let get more details about data type\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see null stats about our data\n",
    "# source : https://stackoverflow.com/questions/44413132/count-the-number-of-missing-values-in-a-dataframe-spark\n",
    "Null_stats = df_spark.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_spark.columns))\n",
    "\n",
    "Null_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# list of columns that we're interested in\n",
    "valid_cols = ['cicid', 'i94yr', 'i94mon', 'i94res', 'i94port', 'arrdate',\\\n",
    "                  'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'gender',\\\n",
    "                  'airline', 'visatype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark = df_spark.select(valid_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|cicid| i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|gender|airline|visatype|\n",
      "+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  null|   null|      B2|\n",
      "|  7.0|2016.0|   4.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|     M|   null|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|     M|     OS|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  null|     AA|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  null|     AA|      B2|\n",
      "+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94addr|\n",
      "+-------+\n",
      "|     .N|\n",
      "|     RG|\n",
      "|     YH|\n",
      "|     RF|\n",
      "|     FT|\n",
      "|     CI|\n",
      "|     TC|\n",
      "|     SC|\n",
      "|     AZ|\n",
      "|     FI|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check data quality for some columns\n",
    "df_spark.select('i94addr').distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From above exploring, I see a lot of issues with the data :\n",
    "- extra columns that don't have much of value.\n",
    "- some columns have high number of nulls.\n",
    "- inaccurate data types for the columns.\n",
    "- some columns have the mapping code not the actual data.\n",
    "- some comluns contain incorrect data, for example: US sates columns \"i94addr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Cleansing****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. After understaning the dataset, I came up with list of columns that will be used.\n",
    "2. All undesired columns will be removed.\n",
    "3. All rows contain Null values will be dropped.\n",
    "4. Column data type will be converted to the appropriate data type.\n",
    "5. SAS date type will be converted to the correct date type.\n",
    "6. Any incorrect value in \"I94addr\" column will be replaced with \"99\"\n",
    "7. For the column (I94MODE), the codes will be replaced with the values.\n",
    "8. For the column (I94VISA), the codes will be replaced with the values.\n",
    "9. I'll create a new column \"stay_length\" that will calculate the stay length.\n",
    "10. The cleansed data will be written to Parquet file partiationed by States, year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***World Temperature Data*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_tmp = spark.read.csv('raw_data/GlobalLandTemperaturesByState.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645675"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_tmp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01|            25.544|                        1.171| Acre| Brazil|\n",
      "|1855-06-01|            24.228|                        1.103| Acre| Brazil|\n",
      "|1855-07-01|            24.371|                        1.044| Acre| Brazil|\n",
      "|1855-08-01|            25.427|                        1.073| Acre| Brazil|\n",
      "|1855-09-01|            25.675|                        1.014| Acre| Brazil|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_tmp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let get more details about data type\n",
    "spark_tmp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               State|\n",
      "+--------------------+\n",
      "|                Utah|\n",
      "|              Hawaii|\n",
      "|           Minnesota|\n",
      "|                Ohio|\n",
      "|            Arkansas|\n",
      "|              Oregon|\n",
      "|District Of Columbia|\n",
      "|     Georgia (State)|\n",
      "|               Texas|\n",
      "|        North Dakota|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's check data quality for some columns\n",
    "spark_tmp = spark_tmp.filter((spark_tmp.Country == 'United States') & (spark_tmp.dt >= '2011-01-01')).dropDuplicates()\n",
    "spark_tmp.select('State').distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From above exploring, I see a lot of issues with the data :\n",
    "- extra columns that don't have much of value.\n",
    "- Date range is out of my project scope.\n",
    "- I'm only interested in US data\n",
    "- inaccurate data types for the columns.\n",
    "- some comluns contain incorrect data, for example: US sates column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Cleansing****\n",
    "1. I'll filter dataset to include only US data\n",
    "2. I'll filter dataset to include only 2011 data or above.\n",
    "3. I'll drop unwanted columns.\n",
    "4. Columns names will be changed if neccessry, for example : td.\n",
    "4. Two new columns will be extracted from date, month and year.\n",
    "5. I'll check the data quilty for states naming it matches the correct naming, for example Gorgia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***U.S. City Demographic Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Exploring****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dm_spark = spark.read.csv('raw_data/us-cities-demographics.csv', sep = ';', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see how data look like\n",
    "dm_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data types\n",
    "dm_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From above exploring, I see a lot of issues with the data :\n",
    "- extra columns that don't have much of value.\n",
    "- A lot columns' names need to be corrected, and shorten. For example, \"Black or African-American\" -> \"Black\"\n",
    "- The data grouped by city.\n",
    "- inaccurate data types for the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Cleansing****\n",
    "1. I'll drop unwanted columns (Number of Veterans, Average Household Size )\n",
    "2. I'll correct columns names\n",
    "3. Appropriate data types will be imposed.\n",
    "4. I'll povit (Race) column then some its count.\n",
    "5. My final data will be grouped by state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Airport Code Table***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Exploring****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_air = spark.read.csv('raw_data/airport-codes_csv.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_air.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see how our data look like\n",
    "spark_air.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# US data and columns\n",
    "spark_air = spark_air.filter(spark_air.iso_country == 'US').select(['ident', 'type', 'name', 'elevation_ft', 'iso_region', 'municipality', 'iata_code', 'local_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "|ident|         type|                name|elevation_ft|iso_region|municipality|iata_code|local_code|\n",
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|     US-PA|    Bensalem|     null|       00A|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|     US-KS|       Leoti|     null|      00AA|\n",
      "| 00AK|small_airport|        Lowell Field|         450|     US-AK|Anchor Point|     null|      00AK|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|     US-AL|     Harvest|     null|      00AL|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|     US-AR|     Newport|     null|      null|\n",
      "+-----+-------------+--------------------+------------+----------+------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_air.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+----------+------------+---------+----------+\n",
      "|ident|type|name|elevation_ft|iso_region|municipality|iata_code|local_code|\n",
      "+-----+----+----+------------+----------+------------+---------+----------+\n",
      "|    0|   0|   0|         239|         0|         102|    20738|      1521|\n",
      "+-----+----+----+------------+----------+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see Null stats\n",
    "spark_air_stats = spark_air.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in spark_air.columns))\n",
    "\n",
    "spark_air_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "| large_airport|\n",
      "|   balloonport|\n",
      "| seaplane_base|\n",
      "|      heliport|\n",
      "|        closed|\n",
      "|medium_airport|\n",
      "| small_airport|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# airports type\n",
    "spark_air.select('type').distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|iso_region|\n",
      "+----------+\n",
      "|     US-TN|\n",
      "|     US-OK|\n",
      "|     US-VT|\n",
      "|     US-SD|\n",
      "|     US-WA|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can we extract US states code from iso_region ?\n",
    "spark_air.select('iso_region').distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From above exploring, I see a lot of issues with the data :\n",
    "- extra columns that don't have much of value.\n",
    "- We only need US data.\n",
    "- US states codes need to be extracted.\n",
    "- inaccurate data types for the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Cleansing****\n",
    "1. I'll drop unwanted columns (continent, coordinates )\n",
    "2. i'll extract US states code from ISO_region column\n",
    "3. Appropriate data types will be imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Our data model is showen below :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/data_model_updated.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I have 8 tables in my data lake :\n",
    "<img src=\"images/table_desc.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run ETL to Model the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### ETL/Pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I used Python, Apache Spark and Airflow to build and run my data pipelines that basaiclly translate my onceptual data model to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/Airflow - DAGs.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- *`airflow/dasg/data_lake_dag.py`* ; contain all airflow tasks to run our data lake pipelines.\n",
    "- *`airflow/plugins/helpers/data_lake_etl.py`* : our data lake pipeline actual code written in python utlizing Pyspark to creat Spark jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/airflow_etl_desc.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- To run pipeline, execute *`airflow/dasg/data_lake_dag.py`* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Data Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Please refere to file **`Data-Dictionary.xlsx`** for data dictioanry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****Tools and Technologies****\n",
    "\n",
    "- **`AWS S3`** : it's very highlly scalabe, reliable, fast, and inexpensive data storage. It's best choice for our data lake from both cost and performance perspective. \n",
    "- **`AWS EMR Spark cluster`** : Apache Spark is a powerful, distributed data in-memory processing engine. With AWS EMR cluster, setting up spark cluster is very easy and will elimnate all time-consuming configration.\n",
    "- **`Apache Airflow`** : it's a great and simple yet a powerful open source tool to automate our data pipeline's sechdulig, runs and mointering.\n",
    "- **`PySpark`** - Python : Pyspark is the official python APIs for Spark. It abstracts all the complexity of spark and provide initiative API to interact with Spark components.\n",
    "- **`Pandas`** : full featured python model to wrok data, and it simplifies data discovery and exploration.\n",
    "- **`Jupyter Notebook`** : great platform to data discovery and exploration, and work with data interactively. \n",
    "- **`Apache Parquet`** : is a very efficient columnar storage format. It provides efficient compression, reduced data storage costs and fast data querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****data update frequency****\n",
    "- **`Monthly`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "****scenarios****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "- **`The data was increased by 100x`**\n",
    "        - Using cloud technologoes like AWS S3 and AWS EMR make it very easy to sacle up our infrastructure to meet our customer demand and expected/unexpcted data volum.\n",
    "        \n",
    "\n",
    "- **`The data populates a dashboard that must be updated on a daily basis by 7am every day`**\n",
    "        - We can schedule our data pipeline via Aiflow to run overnight, and set alerts and SLA to ensure data being updated according to business needs. \n",
    "        \n",
    "        \n",
    "- **`The database needed to be accessed by 100+ people`**\n",
    "        - Again, since our solution is cloud-based it's easy to meet any expect or unexpteced demand. For database, we can add more nodes to scale our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## what's included ?\n",
    "- **`dend.ipynb`** : my project details and write-up\n",
    "- **`airflow`** : contain my data lake code and airflow dags\n",
    "- **`dev`** : it's my developemnt folder, contains all my starting codes and tries.\n",
    "- **`images`** : all images used in my project\n",
    "- **`raw_data`** : contains the inital raw data before being moved to S3 bucket \n",
    "- **`sas_data`** : processed sas data in Parquet format\n",
    "- **`Data-Dictionar.xls`** : our data dictionary in excel format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
